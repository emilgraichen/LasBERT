{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SvY_HLXzZmNb",
        "outputId": "b0bef3d2-9fc2-4250-997e-4fe7d5741011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.18.5\n",
            "  Using cached numpy-1.18.5.zip (5.4 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [190 lines of output]\n",
            "      Running from numpy source directory.\n",
            "      <string>:461: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates\n",
            "      Cythonizing sources\n",
            "      Processing numpy/random\\_bounded_integers.pxd.in\n",
            "      Processing numpy/random\\mtrand.pyx\n",
            "      C:\\Users\\emilg\\AppData\\Local\\Temp\\pip-install-pggarcs4\\numpy_d949c73b4aa64eed97942b6f452ede56\\tools\\cythonize.py:75: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "        required_version = LooseVersion('0.29.14')\n",
            "      C:\\Users\\emilg\\AppData\\Local\\Temp\\pip-install-pggarcs4\\numpy_d949c73b4aa64eed97942b6f452ede56\\tools\\cythonize.py:77: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "        if LooseVersion(cython_version) < required_version:\n",
            "      Processing numpy/random\\_bit_generator.pyx\n",
            "      Processing numpy/random\\_bounded_integers.pyx.in\n",
            "      Processing numpy/random\\_common.pyx\n",
            "      Processing numpy/random\\_generator.pyx\n",
            "      Processing numpy/random\\_mt19937.pyx\n",
            "      Processing numpy/random\\_pcg64.pyx\n",
            "      Processing numpy/random\\_philox.pyx\n",
            "      Processing numpy/random\\_sfc64.pyx\n",
            "      blas_opt_info:\n",
            "      blas_mkl_info:\n",
            "      No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils\n",
            "      customize MSVCCompiler\n",
            "        libraries mkl_rt not found in ['c:\\\\Users\\\\emilg\\\\OneDrive\\\\Dokument\\\\KogVet\\\\729G40 Kandidatuppsats\\\\GPT\\\\.venv\\\\lib', 'C:\\\\']\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      blis_info:\n",
            "        libraries blis not found in ['c:\\\\Users\\\\emilg\\\\OneDrive\\\\Dokument\\\\KogVet\\\\729G40 Kandidatuppsats\\\\GPT\\\\.venv\\\\lib', 'C:\\\\']\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      openblas_info:\n",
            "        libraries openblas not found in ['c:\\\\Users\\\\emilg\\\\OneDrive\\\\Dokument\\\\KogVet\\\\729G40 Kandidatuppsats\\\\GPT\\\\.venv\\\\lib', 'C:\\\\']\n",
            "      get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'\n",
            "      customize GnuFCompiler\n",
            "      Could not locate executable g77\n",
            "      Could not locate executable f77\n",
            "      customize IntelVisualFCompiler\n",
            "      Could not locate executable ifort\n",
            "      Could not locate executable ifl\n",
            "      customize AbsoftFCompiler\n",
            "      Could not locate executable f90\n",
            "      customize CompaqVisualFCompiler\n",
            "      Could not locate executable DF\n",
            "      customize IntelItaniumVisualFCompiler\n",
            "      Could not locate executable efl\n",
            "      customize Gnu95FCompiler\n",
            "      Could not locate executable gfortran\n",
            "      Could not locate executable f95\n",
            "      customize G95FCompiler\n",
            "      Could not locate executable g95\n",
            "      customize IntelEM64VisualFCompiler\n",
            "      customize IntelEM64TFCompiler\n",
            "      Could not locate executable efort\n",
            "      Could not locate executable efc\n",
            "      customize PGroupFlangCompiler\n",
            "      Could not locate executable flang\n",
            "      don't know how to compile Fortran code on platform 'nt'\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      atlas_3_10_blas_threads_info:\n",
            "      Setting PTATLAS=ATLAS\n",
            "        libraries tatlas not found in ['c:\\\\Users\\\\emilg\\\\OneDrive\\\\Dokument\\\\KogVet\\\\729G40 Kandidatuppsats\\\\GPT\\\\.venv\\\\lib', 'C:\\\\']\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      atlas_3_10_blas_info:\n",
            "        libraries satlas not found in ['c:\\\\Users\\\\emilg\\\\OneDrive\\\\Dokument\\\\KogVet\\\\729G40 Kandidatuppsats\\\\GPT\\\\.venv\\\\lib', 'C:\\\\']\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      atlas_blas_threads_info:\n",
            "      Setting PTATLAS=ATLAS\n",
            "        libraries ptf77blas,ptcblas,atlas not found in ['c:\\\\Users\\\\emilg\\\\OneDrive\\\\Dokument\\\\KogVet\\\\729G40 Kandidatuppsats\\\\GPT\\\\.venv\\\\lib', 'C:\\\\']\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      atlas_blas_info:\n",
            "        libraries f77blas,cblas,atlas not found in ['c:\\\\Users\\\\emilg\\\\OneDrive\\\\Dokument\\\\KogVet\\\\729G40 Kandidatuppsats\\\\GPT\\\\.venv\\\\lib', 'C:\\\\']\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      accelerate_info:\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      C:\\Users\\emilg\\AppData\\Local\\Temp\\pip-install-pggarcs4\\numpy_d949c73b4aa64eed97942b6f452ede56\\numpy\\distutils\\system_info.py:1896: UserWarning:\n",
            "          Optimized (vendor) Blas libraries are not found.\n",
            "          Falls back to netlib Blas library which has worse performance.\n",
            "          A better performance should be easily gained by switching\n",
            "          Blas library.\n",
            "        if self._calc_info(blas):\n",
            "      blas_info:\n",
            "        libraries blas not found in ['c:\\\\Users\\\\emilg\\\\OneDrive\\\\Dokument\\\\KogVet\\\\729G40 Kandidatuppsats\\\\GPT\\\\.venv\\\\lib', 'C:\\\\']\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      C:\\Users\\emilg\\AppData\\Local\\Temp\\pip-install-pggarcs4\\numpy_d949c73b4aa64eed97942b6f452ede56\\numpy\\distutils\\system_info.py:1896: UserWarning:\n",
            "          Blas (http://www.netlib.org/blas/) libraries not found.\n",
            "          Directories to search for the libraries can be specified in the\n",
            "          numpy/distutils/site.cfg file (section [blas]) or by setting\n",
            "          the BLAS environment variable.\n",
            "        if self._calc_info(blas):\n",
            "      blas_src_info:\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      C:\\Users\\emilg\\AppData\\Local\\Temp\\pip-install-pggarcs4\\numpy_d949c73b4aa64eed97942b6f452ede56\\numpy\\distutils\\system_info.py:1896: UserWarning:\n",
            "          Blas (http://www.netlib.org/blas/) sources not found.\n",
            "          Directories to search for the sources can be specified in the\n",
            "          numpy/distutils/site.cfg file (section [blas_src]) or by setting\n",
            "          the BLAS_SRC environment variable.\n",
            "        if self._calc_info(blas):\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      non-existing path in 'numpy\\\\distutils': 'site.cfg'\n",
            "      lapack_opt_info:\n",
            "      lapack_mkl_info:\n",
            "        libraries mkl_rt not found in ['c:\\\\Users\\\\emilg\\\\OneDrive\\\\Dokument\\\\KogVet\\\\729G40 Kandidatuppsats\\\\GPT\\\\.venv\\\\lib', 'C:\\\\']\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      openblas_lapack_info:\n",
            "        libraries openblas not found in ['c:\\\\Users\\\\emilg\\\\OneDrive\\\\Dokument\\\\KogVet\\\\729G40 Kandidatuppsats\\\\GPT\\\\.venv\\\\lib', 'C:\\\\']\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      openblas_clapack_info:\n",
            "        libraries openblas,lapack not found in ['c:\\\\Users\\\\emilg\\\\OneDrive\\\\Dokument\\\\KogVet\\\\729G40 Kandidatuppsats\\\\GPT\\\\.venv\\\\lib', 'C:\\\\']\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      flame_info:\n",
            "        libraries flame not found in ['c:\\\\Users\\\\emilg\\\\OneDrive\\\\Dokument\\\\KogVet\\\\729G40 Kandidatuppsats\\\\GPT\\\\.venv\\\\lib', 'C:\\\\']\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      atlas_3_10_threads_info:\n",
            "      Setting PTATLAS=ATLAS\n",
            "        libraries lapack_atlas not found in c:\\Users\\emilg\\OneDrive\\Dokument\\KogVet\\729G40 Kandidatuppsats\\GPT\\.venv\\lib\n",
            "        libraries tatlas,tatlas not found in c:\\Users\\emilg\\OneDrive\\Dokument\\KogVet\\729G40 Kandidatuppsats\\GPT\\.venv\\lib\n",
            "        libraries lapack_atlas not found in C:\\\n",
            "        libraries tatlas,tatlas not found in C:\\\n",
            "      <class 'numpy.distutils.system_info.atlas_3_10_threads_info'>\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      atlas_3_10_info:\n",
            "        libraries lapack_atlas not found in c:\\Users\\emilg\\OneDrive\\Dokument\\KogVet\\729G40 Kandidatuppsats\\GPT\\.venv\\lib\n",
            "        libraries satlas,satlas not found in c:\\Users\\emilg\\OneDrive\\Dokument\\KogVet\\729G40 Kandidatuppsats\\GPT\\.venv\\lib\n",
            "        libraries lapack_atlas not found in C:\\\n",
            "        libraries satlas,satlas not found in C:\\\n",
            "      <class 'numpy.distutils.system_info.atlas_3_10_info'>\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      atlas_threads_info:\n",
            "      Setting PTATLAS=ATLAS\n",
            "        libraries lapack_atlas not found in c:\\Users\\emilg\\OneDrive\\Dokument\\KogVet\\729G40 Kandidatuppsats\\GPT\\.venv\\lib\n",
            "        libraries ptf77blas,ptcblas,atlas not found in c:\\Users\\emilg\\OneDrive\\Dokument\\KogVet\\729G40 Kandidatuppsats\\GPT\\.venv\\lib\n",
            "        libraries lapack_atlas not found in C:\\\n",
            "        libraries ptf77blas,ptcblas,atlas not found in C:\\\n",
            "      <class 'numpy.distutils.system_info.atlas_threads_info'>\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      atlas_info:\n",
            "        libraries lapack_atlas not found in c:\\Users\\emilg\\OneDrive\\Dokument\\KogVet\\729G40 Kandidatuppsats\\GPT\\.venv\\lib\n",
            "        libraries f77blas,cblas,atlas not found in c:\\Users\\emilg\\OneDrive\\Dokument\\KogVet\\729G40 Kandidatuppsats\\GPT\\.venv\\lib\n",
            "        libraries lapack_atlas not found in C:\\\n",
            "        libraries f77blas,cblas,atlas not found in C:\\\n",
            "      <class 'numpy.distutils.system_info.atlas_info'>\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      lapack_info:\n",
            "        libraries lapack not found in ['c:\\\\Users\\\\emilg\\\\OneDrive\\\\Dokument\\\\KogVet\\\\729G40 Kandidatuppsats\\\\GPT\\\\.venv\\\\lib', 'C:\\\\']\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      C:\\Users\\emilg\\AppData\\Local\\Temp\\pip-install-pggarcs4\\numpy_d949c73b4aa64eed97942b6f452ede56\\numpy\\distutils\\system_info.py:1730: UserWarning:\n",
            "          Lapack (http://www.netlib.org/lapack/) libraries not found.\n",
            "          Directories to search for the libraries can be specified in the\n",
            "          numpy/distutils/site.cfg file (section [lapack]) or by setting\n",
            "          the LAPACK environment variable.\n",
            "        return getattr(self, '_calc_info_{}'.format(name))()\n",
            "      lapack_src_info:\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      C:\\Users\\emilg\\AppData\\Local\\Temp\\pip-install-pggarcs4\\numpy_d949c73b4aa64eed97942b6f452ede56\\numpy\\distutils\\system_info.py:1730: UserWarning:\n",
            "          Lapack (http://www.netlib.org/lapack/) sources not found.\n",
            "          Directories to search for the sources can be specified in the\n",
            "          numpy/distutils/site.cfg file (section [lapack_src]) or by setting\n",
            "          the LAPACK_SRC environment variable.\n",
            "        return getattr(self, '_calc_info_{}'.format(name))()\n",
            "        NOT AVAILABLE\n",
            "      \n",
            "      C:\\Users\\emilg\\AppData\\Local\\Temp\\pip-build-env-uu9b7cs7\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:265: UserWarning: Unknown distribution option: 'define_macros'\n",
            "        warnings.warn(msg)\n",
            "      running dist_info\n",
            "      running build_src\n",
            "      build_src\n",
            "      building py_modules sources\n",
            "      creating build\n",
            "      creating build\\src.win-amd64-3.9\n",
            "      creating build\\src.win-amd64-3.9\\numpy\n",
            "      creating build\\src.win-amd64-3.9\\numpy\\distutils\n",
            "      building library \"npymath\" sources\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: metadata-generation-failed\n",
            "\n",
            "× Encountered error while generating package metadata.\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for details.\n",
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy==3.4.1Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "  Using cached spacy-3.4.1-cp39-cp39-win_amd64.whl (11.8 MB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from spacy==3.4.1) (2.4.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from spacy==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from spacy==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from spacy==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from spacy==3.4.1) (3.0.8)\n",
            "Collecting wasabi<1.1.0,>=0.9.1\n",
            "  Using cached wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from spacy==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from spacy==3.4.1) (23.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from spacy==3.4.1) (2.28.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from spacy==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from spacy==3.4.1) (3.0.12)\n",
            "Requirement already satisfied: setuptools in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from spacy==3.4.1) (58.1.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from spacy==3.4.1) (3.1.2)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4\n",
            "  Using cached pydantic-1.9.2-cp39-cp39-win_amd64.whl (2.0 MB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from spacy==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from spacy==3.4.1) (1.23.5)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Using cached typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from spacy==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from spacy==3.4.1) (8.1.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from pathy>=0.3.5->spacy==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy==3.4.1) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.1) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.1) (3.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.1) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.1) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: colorama in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.4.1) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy==3.4.1) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from jinja2->spacy==3.4.1) (2.1.2)\n",
            "Installing collected packages: wasabi, pydantic, typer, spacy\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.1\n",
            "    Uninstalling wasabi-1.1.1:\n",
            "      Successfully uninstalled wasabi-1.1.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.5\n",
            "    Uninstalling pydantic-1.10.5:\n",
            "      Successfully uninstalled pydantic-1.10.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [WinError 5] Åtkomst nekad: 'C:\\\\Users\\\\emilg\\\\OneDrive\\\\Dokument\\\\KogVet\\\\729G40 Kandidatuppsats\\\\GPT\\\\.venv\\\\Lib\\\\site-packages\\\\~ydantic\\\\annotated_types.cp39-win_amd64.pyd'\n",
            "Check the permissions.\n",
            "\n",
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn==1.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from scikit-learn==1.0) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from scikit-learn==1.0) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from scikit-learn==1.0) (1.23.5)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from scikit-learn==1.0) (1.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: yake in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (0.4.8)\n",
            "Requirement already satisfied: networkx in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from yake) (3.0)\n",
            "Requirement already satisfied: jellyfish in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from yake) (0.9.0)\n",
            "Requirement already satisfied: segtok in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from yake) (1.5.11)\n",
            "Requirement already satisfied: click>=6.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from yake) (8.1.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from yake) (1.23.5)\n",
            "Requirement already satisfied: tabulate in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from yake) (0.9.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from click>=6.0->yake) (0.4.6)\n",
            "Requirement already satisfied: regex in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from segtok->yake) (2022.10.31)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement sv-core-news-sm==3.4.1 (from versions: none)\n",
            "ERROR: No matching distribution found for sv-core-news-sm==3.4.1\n",
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.7Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
            "Requirement already satisfied: tqdm in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from nltk==3.7) (4.64.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from nltk==3.7) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from nltk==3.7) (2022.10.31)\n",
            "Requirement already satisfied: click in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from nltk==3.7) (8.1.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from click->nltk==3.7) (0.4.6)\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed nltk-3.7\n",
            "Requirement already satisfied: sentence_transformers in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (2.2.2)Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from sentence_transformers) (4.26.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from sentence_transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: torchvision in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from sentence_transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from sentence_transformers) (1.0)\n",
            "Requirement already satisfied: scipy in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from sentence_transformers) (3.7)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from sentence_transformers) (0.1.97)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from sentence_transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\n",
            "Requirement already satisfied: requests in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (5.4.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Requirement already satisfied: click in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.14)\n",
            "Requirement already satisfied: transformers in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (4.26.1)Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: requests in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from transformers) (2.28.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from requests->transformers) (3.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\emilg\\onedrive\\dokument\\kogvet\\729g40 kandidatuppsats\\gpt\\.venv\\lib\\site-packages (1.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy==1.18.5\n",
        "%pip install spacy==3.4.1\n",
        "%pip install scikit-learn==1.0\n",
        "\n",
        "%pip install yake\n",
        "%pip install sv-core-news-sm==3.4.1\n",
        "%pip install nltk==3.7\n",
        "\n",
        "%pip install sentence_transformers\n",
        "%pip install transformers\n",
        "%pip install joblib\n",
        "\n",
        "#%pip install typing\n",
        "#%pip install spacy \n",
        "#%pip install yake==0.4.8\n",
        "#%pip install python-Levenshtein==0.12.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Koij6FhlEuB3",
        "outputId": "226fcd8b-f7d2-498c-a841-39fc5c5be598"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\emilg\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\emilg\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import csv\n",
        "import math\n",
        "import re #\n",
        "import os\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "from sentence_transformers import SentenceTransformer #\n",
        "from spacy.lang.sv.examples import sentences \n",
        "from nltk.corpus import stopwords\n",
        "from transformers import AutoTokenizer #\n",
        "from transformers import pipeline #\n",
        "from numpy.linalg import norm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('stopwords') #\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "similaritymodel = SentenceTransformer('KBLab/sentence-bert-swedish-cased')\n",
        "generative_model = 'KBLab/bert-base-swedish-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained('KBLab/bert-base-swedish-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "F5eiASIAIS1E"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(original_sentence, stopwords):\n",
        "  \"\"\"Takes the sentence to be simplified (str) and a list of words to ignore (list). \n",
        "  Returns a list of words that aren't stopwords\"\"\"\n",
        "  \n",
        "  keyword_collection = []\n",
        "  clean_sentence = re.sub(r'[^\\w\\s.]|_', '', original_sentence)\n",
        "\n",
        "  for word in clean_sentence.strip(\"\\n\").lower().split():\n",
        "    if word not in stopwords:\n",
        "      keyword_collection.append(word)\n",
        "\n",
        "  return keyword_collection\n",
        "\n",
        "\n",
        "\n",
        "def word_feature_generation(keywords):\n",
        "  \"\"\"Takes a list of words and generates Frequency and Word_length features for each\n",
        "  word. This is used later in the pipeline for Complex Word Identification and synonym ranking.\"\"\"\n",
        "  \n",
        "  #appends all keywords to a dictionary and assigns an empty feature list to each word\n",
        "  word_features = {}\n",
        "  for word in keywords:\n",
        "      word_features[word] = []\n",
        "\n",
        "  #adds the SUC corpus frequency to the dict word_features\n",
        "  with open(\"datasets\\SucX_freq.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=\";\")\n",
        "    keys = list(word_features.keys())\n",
        "\n",
        "    for row in csv_reader:\n",
        "        for word in keys:\n",
        "            if row[0] == word:\n",
        "                word_features[word].append(math.log(int(row[4]),10))\n",
        "                keys.remove(word)\n",
        "\n",
        "    #adds the length of the word to the dict word_features\n",
        "    for word in word_features:\n",
        "        if len(word_features[word]) == 0:\n",
        "            #if frequency not found in SUCX, assume that the frequency is 1. \n",
        "            word_features[word].append(math.log(1, 10))\n",
        "            #count the number of characters of the word and add to dict word_features\n",
        "            word_features[word].append(len(word))\n",
        "            \n",
        "        else:\n",
        "            word_features[word].append(len(word))\n",
        "\n",
        "  #adds the BloggMix corpus frequency to the dict word_features\n",
        "  with open(\"datasets\\stats_blog_odat.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=\";\")\n",
        "      keys = list(word_features.keys())\n",
        "\n",
        "      for row in csv_reader:\n",
        "          for word in keys:\n",
        "              if row[0] == word:\n",
        "                  word_features[word].append(math.log(int(row[4]),10))\n",
        "                  keys.remove(word)\n",
        "\n",
        "      for word in word_features:\n",
        "          if len(word_features[word]) == 2:\n",
        "              #if frequency not found in BloggMix, assume that the frequency is 1. \n",
        "              word_features[word].append(math.log(1, 10))\n",
        "\n",
        "  #adds the TwitterMix corpus frequency to the features\n",
        "  with open(\"datasets\\stats_twitter.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=\";\")\n",
        "      keys = list(word_features.keys())\n",
        "\n",
        "      for row in csv_reader:\n",
        "          for word in keys:\n",
        "              if row[0] == word:\n",
        "                  word_features[word].append(math.log(int(row[6]),10))\n",
        "                  keys.remove(word)\n",
        "\n",
        "      for word in word_features:\n",
        "          if len(word_features[word]) == 3:\n",
        "              #if frequency not found in TwitterMix, assume that the frequency is 1. \n",
        "              word_features[word].append(math.log(1, 10))   \n",
        "\n",
        "  return word_features\n",
        "\n",
        "\n",
        "\n",
        "def complex_word_identification(word_features):\n",
        "  \"\"\"Takes words and their features (stored in a dict) and returns the complex words (dict).\n",
        "  Every word that the RFC predicts as 3 or 4 are regarded as complex.\"\"\"\n",
        "  complex_words = {}\n",
        "\n",
        "  #load pre-trained Random Forest Classifier\n",
        "  loaded_rf = joblib.load(\"classifiers/random_forest_multi_freq.joblib\")\n",
        "\n",
        "  #classify complexity of words based on the features of the words\n",
        "  for word in word_features:\n",
        "    prediction = loaded_rf.predict([word_features[word]])\n",
        "\n",
        "    #if the word is classified as more than level 2, it is complex\n",
        "    if prediction[0] > 2:\n",
        "      complex_words[word] = prediction[0]\n",
        "\n",
        "  return complex_words\n",
        "\n",
        "\n",
        "\n",
        "def mask_word(original_sentence, complex_word):\n",
        "  \"\"\"Takes the sentence (str) to be simplified and returns an altered sentence (str) \n",
        "  with the current complex word in the sentence masked out.\n",
        "  \"\"\"\n",
        "  split_sentence = original_sentence.lower().strip(\".\").split()\n",
        "\n",
        "  i= 0\n",
        "  for word in split_sentence:\n",
        "    if word.strip(\".\") == complex_word.strip(\".\"):\n",
        "      split_sentence[i] = \"[MASK]\"\n",
        "      break\n",
        "    elif word.strip(\".\") == complex_word.strip(\".\"):\n",
        "      split_sentence[i] = \"[MASK]\"\n",
        "      break\n",
        "    i += 1\n",
        "\n",
        "  masked_string = \" \".join(split_sentence)\n",
        "  \n",
        "  return masked_string\n",
        "\n",
        "\n",
        "def generate_alternatives(masked_sentence, original_sentence):\n",
        "  \"\"\"Generates alternatives for complex, masked word in a sentence.\n",
        "  Takes the masked sentence, original sentence and the text to the left and right\n",
        "  of the original sentence (all str). Returns alternatives (list) to the complex word.\n",
        "  \"\"\"\n",
        "  #concatenate original sentence and [MASK]:ed out sentence\n",
        "  sentence_pair = original_sentence + \" \" + masked_sentence + \".\"\n",
        "\n",
        "  #use generative_model (KBLab/BERT-base-swedish-cased) to predict masked out word\n",
        "  fill_mask = pipeline(task=\"fill-mask\", model=generative_model, tokenizer=tokenizer)\n",
        "\n",
        "  #generate 20 alternatives \n",
        "  preds = fill_mask(sentence_pair, top_k=20)\n",
        "  preds = [\n",
        "      {\n",
        "          \"token_str\": pred[\"token_str\"],\n",
        "      }\n",
        "      for pred in preds\n",
        "  ]\n",
        "\n",
        "  #the predicted alternatives to the complex word are stored\n",
        "  word_alternatives = []\n",
        "  for prediction in preds:\n",
        "    word_alternatives.append(prediction[\"token_str\"])\n",
        "\n",
        "  return word_alternatives\n",
        "\n",
        "\n",
        "def synonym_selection(word_alternatives, target_word, original_sentence):\n",
        "  \"\"\"Selects the most similar alternative of the proposed alternatives to the original \n",
        "  sentence. This is to preserve the meaning of the sentence.\"\"\"\n",
        "\n",
        "  filtered_words = []\n",
        "\n",
        "  #Filtering: checks that both alternative word and complex word has same POS-tag\n",
        "  with open(\"datasets/SucX_freq.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=\";\")\n",
        "\n",
        "    target_word_pos = None\n",
        "    #find complex word POS-tag\n",
        "    for row in csv_reader:\n",
        "      if target_word == row[0]:\n",
        "          target_word_pos = row[1].split(\".\")[0]\n",
        "\n",
        "  if target_word_pos != None:\n",
        "    with open(\"datasets/SucX_freq.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=\";\")\n",
        "      for word in word_alternatives:\n",
        "        ##removes words that begin with \"##\"\n",
        "        if word.startswith(\"##\"):\n",
        "          continue\n",
        "        \n",
        "        #find alternative word POS-tag\n",
        "        for row in csv_reader:\n",
        "            if row[0] == word:\n",
        "                if row[1].split(\".\")[0] == target_word_pos:\n",
        "                  filtered_words.append(word)\n",
        "                  break\n",
        "        csv_file.seek(0) \n",
        "\n",
        "  #splits the original sentence into words\n",
        "  words = original_sentence.strip(\".\").split()\n",
        "  sentence_alternatives = []\n",
        "\n",
        "  #create alternative sentences with the target word replaced\n",
        "  for i, word in enumerate(words):\n",
        "    if word == target_word:\n",
        "      for replacement_word in filtered_words:\n",
        "        # Create a copy of the original list of words\n",
        "        new_words = words.copy()\n",
        "        # Replace the target word with the replacement word\n",
        "        new_words[i] = replacement_word\n",
        "        # Join the words back together into a sentence\n",
        "        new_sentence = ' '.join(new_words)\n",
        "        # Add the new sentence to the list of new sentences\n",
        "        sentence_alternatives.append((replacement_word, new_sentence))\n",
        "\n",
        "  #encode alternative sentences\n",
        "  embeddings = []\n",
        "  for sentence in sentence_alternatives:\n",
        "    embeddings.append(similaritymodel.encode(sentence[1]))\n",
        "\n",
        "  #combine sentence and corresponding embedding into list of pairs\n",
        "  sentence_embeddings = list(zip(sentence_alternatives, embeddings))\n",
        "\n",
        "  #encode the original sentence\n",
        "  original_sentence = similaritymodel.encode(original_sentence)\n",
        "  A = original_sentence\n",
        "\n",
        "  #find the most similar sentence to the original, using cosine distance\n",
        "  synonymy = {}\n",
        "  for i in range (0, len(sentence_embeddings)):\n",
        "    B = sentence_embeddings[i][1]\n",
        "    cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
        "    \n",
        "    synonymy[sentence_embeddings[i][0][0]] = cosine\n",
        "    \n",
        "\n",
        "  #sorts the synonyms according to similarity\n",
        "  #reverse=true indicates that most similar is ordered first\n",
        "  ordered_synonymy = sorted(synonymy, key=synonymy.get, reverse=True)\n",
        "\n",
        "  return ordered_synonymy[0:4]\n",
        "  \n",
        "\n",
        "\n",
        "def synonym_ranking(suitable_synonyms, target_word, original_sentence):\n",
        "  \"\"\"Ranks suitable synonyms according to simplicity\"\"\"\n",
        "\n",
        "  #loads word complexity classifier-system (the same used in the CWI subtask)\n",
        "  loaded_rf = joblib.load(\"classifiers/random_forest_multi_freq.joblib\")\n",
        "  synonym_simplicity = {}\n",
        "  \n",
        "  #generates features for the alternatives and the target word\n",
        "  synonym_features = word_feature_generation([target_word] + suitable_synonyms)\n",
        "\n",
        "  #predict complexity of suitable synonyms\n",
        "  for synonym in synonym_features:\n",
        "    features = [synonym_features[synonym]]\n",
        "    prediction = loaded_rf.predict(features)\n",
        "    synonym_simplicity[synonym] = prediction\n",
        "  \n",
        "  \n",
        "  synonym_simplicity = sorted(synonym_simplicity, key=synonym_simplicity.get)\n",
        "\n",
        "  if len(synonym_simplicity) > 0:\n",
        "    #replaces complex word with synonymous word\n",
        "    split_sentence = original_sentence.lower().split(\" \")\n",
        "    \n",
        "    i = 0\n",
        "    for word in split_sentence:\n",
        "      if word.strip(\".\") == target_word:\n",
        "        split_sentence[i] = synonym_simplicity[0]\n",
        "      i += 1\n",
        "\n",
        "    preferred_sentence = \" \".join(split_sentence)\n",
        "\n",
        "    return preferred_sentence.strip(\" \")\n",
        "  return str(original_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simplify(sentence):\n",
        "    original_sentence = sentence\n",
        "    #remove stopwords the keywords of the text\n",
        "    keywords = remove_stopwords(sentence, stopwords.words(\"swedish\"))\n",
        "    #generate the features of the keywords (word_frequency, POS, word length)\n",
        "    keyword_features = word_feature_generation(keywords)\n",
        "    #use a random forest classifier to find the complex words in each sentence\n",
        "    complex_words = complex_word_identification(keyword_features)\n",
        "\n",
        "    #iterative simplification\n",
        "    for complex_word in complex_words:\n",
        "      #mask the complex word from the sentence\n",
        "      masked_sentence = mask_word(sentence, complex_word)\n",
        "      #generate alternatives to the complex word with BERT\n",
        "      alternative_words = generate_alternatives(masked_sentence, original_sentence)\n",
        "      #find the 5 most suitable synonyms \n",
        "      suitable_synonyms = synonym_selection(alternative_words, complex_word, sentence)\n",
        "      #rank the suitable synonyms according to their simplicity with random forest classifier\n",
        "      sentence = synonym_ranking(suitable_synonyms, complex_word, sentence)\n",
        "      \n",
        "    print(\"THE ORIGINAL SENTENCE IS: \", original_sentence)\n",
        "    print(\"THE SIMPLIFIED SENTENCE IS: \", sentence)\n",
        "    #print(\"--------------------------------\")\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "THE ORIGINAL SENTENCE IS:  Jag tycker hon är en exeptionellt förnämlig tonsättare.\n",
            "THE SIMPLIFIED SENTENCE IS:  jag tycker hon är en exeptionellt fin tonsättare.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'jag tycker hon är en exeptionellt fin tonsättare.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simplify(\"Jag tycker hon är en exeptionellt förnämlig tonsättare.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n",
            "Wrote to file!\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  folder_path = \"texts\"\n",
        "  paragraph = \"\"\n",
        "  index = 0\n",
        "\n",
        "  with open('results-file.csv', mode='w', newline='', encoding=\"utf-8\") as file:\n",
        "      # Create a CSV writer object\n",
        "      writer = csv.writer(file)\n",
        "      # Write the header row\n",
        "      writer.writerow(['Index', 'Original Sentence', 'Altered Sentence BERT',\"Replacements\", \"Complex Word\", \"Complex Word Frequency\", \"Synonyms\"])\n",
        "\n",
        "  with open('dataset-file.csv', 'r', encoding=(\"utf-8\")) as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=\";\")\n",
        "\n",
        "    n = 0\n",
        "      # Loop through each row in the CSV file\n",
        "    for row in csv_reader:\n",
        "        # Get the sentence from the fourth column of the current row\n",
        "        # Call the simplify() function on the sentence\n",
        "        sentence = re.sub(r'[^\\w\\s.]|_', '', row[3])\n",
        "        target_word = row[0]\n",
        "        simplified_sentence = simplify(sentence)\n",
        "\n",
        "        write_sentences = []\n",
        "        index += n\n",
        "        write_sentences.append(\n",
        "        {\n",
        "            \"index\": index,\n",
        "            \"original_sentence\": str(sentence),\n",
        "            \"altered_sentence_BERT\": simplified_sentence,\n",
        "            \"complex_word\": target_word,\n",
        "            \"complex_word_frequency\": row[1],\n",
        "            \"synonyms\": row[2]\n",
        "        })\n",
        "\n",
        "        # Open the results-CSV file in write mode\n",
        "        with open('results-file.csv', mode='a', newline='', encoding=\"utf-8\") as file:\n",
        "          # Create a CSV writer object\n",
        "          writer = csv.writer(file)\n",
        "          # Write the data rows\n",
        "          for sentence in write_sentences:\n",
        "              replacements = {}\n",
        "              for i, word in enumerate(re.sub(r'[^\\w\\s.]|_', '', sentence[\"original_sentence\"].lower()).split(\" \")): \n",
        "                if word not in re.sub(r'[^\\w\\s.]|_', '', sentence[\"altered_sentence_BERT\"].lower()).split(\" \"):\n",
        "                    try:\n",
        "                      replacements[word] = sentence[\"altered_sentence_BERT\"].lower().split(\" \")[i]\n",
        "                    except IndexError:\n",
        "                      continue\n",
        "              writer.writerow([sentence[\"index\"], sentence[\"original_sentence\"], sentence[\"altered_sentence_BERT\"], replacements, sentence[\"complex_word\"], sentence[\"complex_word_frequency\"], sentence[\"synonyms\"]])\n",
        "              print(\"Wrote to file!\")\n",
        "    \n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install ipywidgets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(\"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\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "d64ec9afa9a4528c53ec1cbc89c36dabcfd9570d5e3837213de1dcdf9464225a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
