{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SvY_HLXzZmNb",
        "outputId": "b0bef3d2-9fc2-4250-997e-4fe7d5741011"
      },
      "outputs": [],
      "source": [
        "%pip install numpy==1.18.5\n",
        "%pip install spacy==3.4.1\n",
        "%pip install scikit-learn==1.0\n",
        "\n",
        "%pip install yake\n",
        "%pip install sv-core-news-sm==3.4.1\n",
        "%pip install nltk==3.7\n",
        "\n",
        "%pip install sentence_transformers\n",
        "%pip install transformers\n",
        "%pip install joblib"
      ]
    },
    ,
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import csv\n",
        "import math\n",
        "import re \n",
        "import os\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "from sentence_transformers import SentenceTransformer \n",
        "from spacy.lang.sv.examples import sentences \n",
        "from nltk.corpus import stopwords\n",
        "from transformers import AutoTokenizer \n",
        "from transformers import pipeline \n",
        "from numpy.linalg import norm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('stopwords') \n",
        "#nltk.download('punkt')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "similaritymodel = SentenceTransformer('KBLab/sentence-bert-swedish-cased')\n",
        "generative_model = 'KBLab/bert-base-swedish-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained('KBLab/bert-base-swedish-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "F5eiASIAIS1E"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(original_sentence, stopwords):\n",
        "  \"\"\"Takes the sentence to be simplified (str) and a list of words to ignore (list). \n",
        "  Returns a list of words that aren't stopwords\"\"\"\n",
        "  \n",
        "  keyword_collection = []\n",
        "  clean_sentence = re.sub(r'[^\\w\\s.]|_', '', original_sentence)\n",
        "\n",
        "  for word in clean_sentence.strip(\"\\n\").lower().split():\n",
        "    if word not in stopwords:\n",
        "      keyword_collection.append(word)\n",
        "\n",
        "  return keyword_collection\n",
        "\n",
        "\n",
        "\n",
        "def word_feature_generation(keywords):\n",
        "  \"\"\"Takes a list of words and generates Frequency and Word_length features for each\n",
        "  word. This is used later in the pipeline for Complex Word Identification and synonym ranking.\"\"\"\n",
        "  \n",
        "  #appends all keywords to a dictionary and assigns an empty feature list to each word\n",
        "  word_features = {}\n",
        "  for word in keywords:\n",
        "      word_features[word] = []\n",
        "\n",
        "  #adds the SUC corpus frequency to the dict word_features\n",
        "  with open(\"datasets\\SucX_freq.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=\";\")\n",
        "    keys = list(word_features.keys())\n",
        "\n",
        "    for row in csv_reader:\n",
        "        for word in keys:\n",
        "            if row[0] == word:\n",
        "                word_features[word].append(math.log(int(row[4]),10))\n",
        "                keys.remove(word)\n",
        "\n",
        "    #adds the length of the word to the dict word_features\n",
        "    for word in word_features:\n",
        "        if len(word_features[word]) == 0:\n",
        "            #if frequency not found in SUCX, assume that the frequency is 1. \n",
        "            word_features[word].append(math.log(1, 10))\n",
        "            #count the number of characters of the word and add to dict word_features\n",
        "            word_features[word].append(len(word))\n",
        "            \n",
        "        else:\n",
        "            word_features[word].append(len(word))\n",
        "\n",
        "  #adds the BloggMix corpus frequency to the dict word_features\n",
        "  with open(\"datasets\\stats_blog_odat.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=\";\")\n",
        "      keys = list(word_features.keys())\n",
        "\n",
        "      for row in csv_reader:\n",
        "          for word in keys:\n",
        "              if row[0] == word:\n",
        "                  word_features[word].append(math.log(int(row[4]),10))\n",
        "                  keys.remove(word)\n",
        "\n",
        "      for word in word_features:\n",
        "          if len(word_features[word]) == 2:\n",
        "              #if frequency not found in BloggMix, assume that the frequency is 1. \n",
        "              word_features[word].append(math.log(1, 10))\n",
        "\n",
        "  #adds the TwitterMix corpus frequency to the features\n",
        "  with open(\"datasets\\stats_twitter.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=\";\")\n",
        "      keys = list(word_features.keys())\n",
        "\n",
        "      for row in csv_reader:\n",
        "          for word in keys:\n",
        "              if row[0] == word:\n",
        "                  word_features[word].append(math.log(int(row[6]),10))\n",
        "                  keys.remove(word)\n",
        "\n",
        "      for word in word_features:\n",
        "          if len(word_features[word]) == 3:\n",
        "              #if frequency not found in TwitterMix, assume that the frequency is 1. \n",
        "              word_features[word].append(math.log(1, 10))   \n",
        "\n",
        "  return word_features\n",
        "\n",
        "\n",
        "\n",
        "def complex_word_identification(word_features):\n",
        "  \"\"\"Takes words and their features (stored in a dict) and returns the complex words (dict).\n",
        "  Every word that the RFC predicts as 3 or 4 are regarded as complex.\"\"\"\n",
        "  complex_words = {}\n",
        "\n",
        "  #load pre-trained Random Forest Classifier\n",
        "  loaded_rf = joblib.load(\"classifiers/random_forest_multi_freq.joblib\")\n",
        "\n",
        "  #classify complexity of words based on the features of the words\n",
        "  for word in word_features:\n",
        "    prediction = loaded_rf.predict([word_features[word]])\n",
        "\n",
        "    #if the word is classified as more than level 2, it is complex\n",
        "    if prediction[0] > 2:\n",
        "      complex_words[word] = prediction[0]\n",
        "\n",
        "  return complex_words\n",
        "\n",
        "\n",
        "\n",
        "def mask_word(original_sentence, complex_word):\n",
        "  \"\"\"Takes the sentence (str) to be simplified and returns an altered sentence (str) \n",
        "  with the current complex word in the sentence masked out.\n",
        "  \"\"\"\n",
        "  split_sentence = original_sentence.lower().strip(\".\").split()\n",
        "\n",
        "  i= 0\n",
        "  for word in split_sentence:\n",
        "    if word.strip(\".\") == complex_word.strip(\".\"):\n",
        "      split_sentence[i] = \"[MASK]\"\n",
        "      break\n",
        "    elif word.strip(\".\") == complex_word.strip(\".\"):\n",
        "      split_sentence[i] = \"[MASK]\"\n",
        "      break\n",
        "    i += 1\n",
        "\n",
        "  masked_string = \" \".join(split_sentence)\n",
        "  \n",
        "  return masked_string\n",
        "\n",
        "\n",
        "def generate_alternatives(masked_sentence, original_sentence):\n",
        "  \"\"\"Generates alternatives for complex, masked word in a sentence.\n",
        "  Takes the masked sentence, original sentence and the text to the left and right\n",
        "  of the original sentence (all str). Returns alternatives (list) to the complex word.\n",
        "  \"\"\"\n",
        "  #concatenate original sentence and [MASK]:ed out sentence\n",
        "  sentence_pair = original_sentence + \" \" + masked_sentence + \".\"\n",
        "\n",
        "  #use generative_model (KBLab/BERT-base-swedish-cased) to predict masked out word\n",
        "  fill_mask = pipeline(task=\"fill-mask\", model=generative_model, tokenizer=tokenizer)\n",
        "\n",
        "  #generate 20 alternatives \n",
        "  preds = fill_mask(sentence_pair, top_k=20)\n",
        "  preds = [{\"token_str\": pred[\"token_str\"]} for pred in preds]\n",
        "\n",
        "  #the predicted alternatives to the complex word are stored\n",
        "  word_alternatives = []\n",
        "  for prediction in preds:\n",
        "    word_alternatives.append(prediction[\"token_str\"])\n",
        "\n",
        "  return word_alternatives\n",
        "\n",
        "\n",
        "def synonym_selection(word_alternatives, target_word, original_sentence):\n",
        "  \"\"\"Selects the most similar alternative of the proposed alternatives to the original \n",
        "  sentence. This is to preserve the meaning of the sentence.\"\"\"\n",
        "\n",
        "  filtered_words = []\n",
        "\n",
        "  #Filtering: checks that both alternative word and complex word has same POS-tag\n",
        "  with open(\"datasets/SucX_freq.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=\";\")\n",
        "\n",
        "    target_word_pos = None\n",
        "    #find complex word POS-tag\n",
        "    for row in csv_reader:\n",
        "      if target_word == row[0]:\n",
        "          target_word_pos = row[1].split(\".\")[0]\n",
        "\n",
        "  if target_word_pos != None:\n",
        "    with open(\"datasets/SucX_freq.csv\", \"r\", encoding=\"utf-8\") as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=\";\")\n",
        "      for word in word_alternatives:\n",
        "        ##removes words that begin with \"##\"\n",
        "        if word.startswith(\"##\"):\n",
        "          continue\n",
        "        \n",
        "        #find alternative word POS-tag\n",
        "        for row in csv_reader:\n",
        "            if row[0] == word:\n",
        "                if row[1].split(\".\")[0] == target_word_pos:\n",
        "                  filtered_words.append(word)\n",
        "                  break\n",
        "        csv_file.seek(0) \n",
        "\n",
        "  #splits the original sentence into words\n",
        "  words = original_sentence.strip(\".\").split()\n",
        "  sentence_alternatives = []\n",
        "\n",
        "  #create alternative sentences with the target word replaced\n",
        "  for i, word in enumerate(words):\n",
        "    if word == target_word:\n",
        "      for replacement_word in filtered_words:\n",
        "        # Create a copy of the original list of words\n",
        "        new_words = words.copy()\n",
        "        # Replace the target word with the replacement word\n",
        "        new_words[i] = replacement_word\n",
        "        # Join the words back together into a sentence\n",
        "        new_sentence = ' '.join(new_words)\n",
        "        # Add the new sentence to the list of new sentences\n",
        "        sentence_alternatives.append((replacement_word, new_sentence))\n",
        "\n",
        "  #encode alternative sentences\n",
        "  embeddings = []\n",
        "  for sentence in sentence_alternatives:\n",
        "    embeddings.append(similaritymodel.encode(sentence[1]))\n",
        "\n",
        "  #combine sentence and corresponding embedding into list of pairs\n",
        "  sentence_embeddings = list(zip(sentence_alternatives, embeddings))\n",
        "\n",
        "  #encode the original sentence\n",
        "  original_sentence = similaritymodel.encode(original_sentence)\n",
        "  A = original_sentence\n",
        "\n",
        "  #find the most similar sentence to the original, using cosine distance\n",
        "  synonymy = {}\n",
        "  for i in range (0, len(sentence_embeddings)):\n",
        "    B = sentence_embeddings[i][1]\n",
        "    cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
        "    \n",
        "    synonymy[sentence_embeddings[i][0][0]] = cosine\n",
        "    \n",
        "\n",
        "  #sorts the synonyms according to similarity\n",
        "  #reverse=true indicates that most similar is ordered first\n",
        "  ordered_synonymy = sorted(synonymy, key=synonymy.get, reverse=True)\n",
        "\n",
        "  return ordered_synonymy[0:4]\n",
        "  \n",
        "\n",
        "\n",
        "def synonym_ranking(suitable_synonyms, target_word, original_sentence):\n",
        "  \"\"\"Ranks suitable synonyms according to simplicity\"\"\"\n",
        "\n",
        "  #loads word complexity classifier-system (the same used in the CWI subtask)\n",
        "  loaded_rf = joblib.load(\"classifiers/random_forest_multi_freq.joblib\")\n",
        "  synonym_simplicity = {}\n",
        "  \n",
        "  #generates features for the alternatives and the target word\n",
        "  synonym_features = word_feature_generation([target_word] + suitable_synonyms)\n",
        "\n",
        "  #predict complexity of suitable synonyms\n",
        "  for synonym in synonym_features:\n",
        "    features = [synonym_features[synonym]]\n",
        "    prediction = loaded_rf.predict(features)\n",
        "    synonym_simplicity[synonym] = prediction\n",
        "  \n",
        "  \n",
        "  synonym_simplicity = sorted(synonym_simplicity, key=synonym_simplicity.get)\n",
        "\n",
        "  if len(synonym_simplicity) > 0:\n",
        "    #replaces complex word with synonymous word\n",
        "    split_sentence = original_sentence.lower().split(\" \")\n",
        "    \n",
        "    i = 0\n",
        "    for word in split_sentence:\n",
        "      if word.strip(\".\") == target_word:\n",
        "        split_sentence[i] = synonym_simplicity[0]\n",
        "      i += 1\n",
        "\n",
        "    preferred_sentence = \" \".join(split_sentence)\n",
        "\n",
        "    return preferred_sentence.strip(\" \")\n",
        "  return str(original_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simplify(sentence):\n",
        "    original_sentence = sentence\n",
        "    #remove stopwords the keywords of the text\n",
        "    keywords = remove_stopwords(sentence, stopwords.words(\"swedish\"))\n",
        "    #generate the features of the keywords (word_frequency, POS, word length)\n",
        "    keyword_features = word_feature_generation(keywords)\n",
        "    #use a random forest classifier to find the complex words in each sentence\n",
        "    complex_words = complex_word_identification(keyword_features)\n",
        "\n",
        "    #iterative simplification\n",
        "    for complex_word in complex_words:\n",
        "      #mask the complex word from the sentence\n",
        "      masked_sentence = mask_word(sentence, complex_word)\n",
        "      #generate alternatives to the complex word with BERT\n",
        "      alternative_words = generate_alternatives(masked_sentence, original_sentence)\n",
        "      #find the 5 most suitable synonyms \n",
        "      suitable_synonyms = synonym_selection(alternative_words, complex_word, sentence)\n",
        "      #rank the suitable synonyms according to their simplicity with random forest classifier\n",
        "      sentence = synonym_ranking(suitable_synonyms, complex_word, sentence)\n",
        "      \n",
        "    print(\"THE ORIGINAL SENTENCE IS: \", original_sentence)\n",
        "    print(\"THE SIMPLIFIED SENTENCE IS: \", sentence)\n",
        "    #print(\"--------------------------------\")\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "THE ORIGINAL SENTENCE IS:  Enligt Artikel 1 får en fördragsstat under inga omständigheter anskaffa eller inneha biologiska vapen\n",
            "THE SIMPLIFIED SENTENCE IS:  enligt artikel 1 får en fördragsstat under inga omständigheter skaffa eller inneha mänskliga vapen\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'enligt artikel 1 får en fördragsstat under inga omständigheter skaffa eller inneha mänskliga vapen'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simplify(\"Enligt Artikel 1 får en fördragsstat under inga omständigheter anskaffa eller inneha biologiska vapen.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "d64ec9afa9a4528c53ec1cbc89c36dabcfd9570d5e3837213de1dcdf9464225a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
